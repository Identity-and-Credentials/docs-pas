---
title: VMware Tanzu Application Service for VMs v2.13 Release Notes
owner: Release Engineering
---

This topic contains release notes for VMware Tanzu Application Service for VMs (TAS for VMs) v2.13.

TAS for VMs is certified by the Cloud Foundry Foundation for <%= Date.today.year %>.

For more information about the Cloud Foundry Certified Provider Program, see
[How Do I Become a Certified Provider?](https://www.cloudfoundry.org/certified-platforms-how-to/)
on the Cloud Foundry website.

Because VMware uses the Percona Distribution for MySQL, expect a time lag between Oracle
releasing a MySQL patch and VMware releasing TAS for VMs containing that patch.

<hr>

## <a id='releases'></a> Releases

<%# DO NOT DELETE THIS LINE - robots use this to add new release notes %>

### <a id='2.13.0'></a> 2.13.0

**Release Date:** March 29, 2022

* See [New Features in TAS for VMs v2.13](#new-features).
* See [Resolved Issues](#resolved-issues).
* See [Breaking Changes](#breaking-changes).
* See [Known Issues](#known-issues).

<table border="1" class="nice">
  <thead>
    <tr>
      <th>Component</th>
      <th>Version</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>ubuntu-xenial stemcell</td><td>621.211</td></tr>
    <tr><td>backup-and-restore-sdk</td><td>1.18.34</td></tr>
    <tr><td>binary-offline-buildpack</td><td>1.0.42</td></tr>
    <tr><td>bosh-dns-aliases</td><td>0.0.4</td></tr>
    <tr><td>bosh-system-metrics-forwarder</td><td>0.0.21</td></tr>
    <tr><td>bpm</td><td>1.1.16</td></tr>
    <tr><td>capi</td><td>1.127.0</td></tr>
    <tr><td>cf-autoscaling</td><td>248</td></tr>
    <tr><td>cf-cli</td><td>1.38.0</td></tr>
    <tr><td>cf-networking</td><td>3.3.0</td></tr>
    <tr><td>cflinuxfs3</td><td>0.274.0</td></tr>
    <tr><td>credhub</td><td>2.9.9</td></tr>
    <tr><td>diego</td><td>2.58.1</td></tr>
    <tr><td>dotnet-core-offline-buildpack</td><td>2.3.38</td></tr>
    <tr><td>garden-runc</td><td>1.20.0</td></tr>
    <tr><td>go-offline-buildpack</td><td>1.9.38</td></tr>
    <tr><td>haproxy</td><td>11.9.3</td></tr>
    <tr><td>java-offline-buildpack</td><td>4.48</td></tr>
    <tr><td>log-cache</td><td>2.11.5</td></tr>
    <tr><td>loggregator</td><td>106.6.2</td></tr>
    <tr><td>loggregator-agent</td><td>6.3.8</td></tr>
    <tr><td>mapfs</td><td>1.2.6</td></tr>
    <tr><td>metric-registrar</td><td>1.2.4</td></tr>
    <tr><td>metrics-discovery</td><td>3.0.8</td></tr>
    <tr><td>mysql-monitoring</td><td>9.15.0</td></tr>
    <tr><td>nats</td><td>43</td></tr>
    <tr><td>nfs-volume</td><td>7.1.1</td></tr>
    <tr><td>nginx-offline-buildpack</td><td>1.1.34</td></tr>
    <tr><td>nodejs-offline-buildpack</td><td>1.7.66</td></tr>
    <tr><td>notifications</td><td>62</td></tr>
    <tr><td>notifications-ui</td><td>40</td></tr>
    <tr><td>php-offline-buildpack</td><td>4.4.55</td></tr>
    <tr><td>push-apps-manager-release</td><td>676.0.1</td></tr>
    <tr><td>push-offline-docs-release</td><td>1.0.33</td></tr>
    <tr><td>push-usage-service-release</td><td>674.0.23</td></tr>
    <tr><td>pxc</td><td>0.41.0</td></tr>
    <tr><td>python-offline-buildpack</td><td>1.7.49</td></tr>
    <tr><td>r-offline-buildpack</td><td>1.1.25</td></tr>
    <tr><td>routing</td><td>0.231.0</td></tr>
    <tr><td>ruby-offline-buildpack</td><td>1.8.50</td></tr>
    <tr><td>silk</td><td>3.3.0</td></tr>
    <tr><td>smb-volume</td><td>3.1.0</td></tr>
    <tr><td>smoke-tests</td><td>4.5.0</td></tr>
    <tr><td>staticfile-offline-buildpack</td><td>1.5.28</td></tr>
    <tr><td>statsd-injector</td><td>1.11.18</td></tr>
    <tr><td>syslog</td><td>11.7.7</td></tr>
    <tr><td>system-metrics-scraper</td><td>3.2.4</td></tr>
    <tr><td>uaa</td><td>74.5.34</td></tr>
  </tbody>
</table>

<%# To do: organize and expand bulleted release notes

* **[Feature]** Allows operators to enable Dynamic ASG updates, so that apps can inherit new and updated ASGs without requiring a restart
* **[Feature]** AppsManager - Ability to update destination protocol
* **[Feature]** Monit thresholds for Cloud Controller Worker are configurable
* **[Feature]** Cloud Controller - Allow operators to configure whether audit events are output to the system logs
* **[Feature]** Configure HTTP/2 Support

* **[Feature Improvement]** AppsManager - CfCli installers now can be downloaded in airgapped envs
* **[Feature Improvement]** Log cache split out into different instance group
* **[Feature Improvement]** Apps manager HTTP2 Support
* **[Feature Improvement]** Cloud Controller - Allow operators to configure the Cloud Controller monit healthcheck timeout
* **[Feature Improvement]** Enable HTTP/2 for HAProxy
* **[Feature Improvement]** Set default for System metrics scrape interval to 15s
* **[Feature Improvement]** HTTP/2 toggle disables Diego container proxy ALPN
* **[Feature Improvement]** Add secure internal network metric scraping for metric registrar

* **[Bug Fix]** Assign cloud_controller.read and cloud_controller.write scopes to service brokers created using CF CLI v8
* **[Bug Fix]** Fix default metric registrar blocked tags to include 'ip' and remove 'id'
* **[Bug Fix]** Fix metric-registrar blocked tags configuration
* **[Bug Fix]** Restore missing networking and garden metrics
* **[Bug Fix]** Smoke tests support for TLSv1.3 only option
* **[Bug Fix]** Smoke Tests uses specified domain for Isolation Segments
* **[Bug Fix]** Enabling audit logging file rotation to reduce IO load during log rotation
* **[Bug Fix]** Fix log cache nozzle metrics
* **[Bug Fix]** Cloud Controller - Ensure app lifecycle_type is not nil when determining app lifecycle
* **[Bug Fix]** Fix certificate rotation by fixing CredHub's import of concatenated certificates
* **[Bug Fix]** Fix "System metrics scrape interval" configuration in manifest%>



## <a id='upgrade'></a> How to Upgrade

To upgrade to TAS for VMs v2.13, see [Configuring TAS for VMs for Upgrades](https://docs.pivotal.io/application-service/2-12/operating/configuring.html).

When upgrading to TAS for VMs v2.13, be aware of the following upgrade considerations:

* If you previously used an earlier version of TAS for VMs, you must first upgrade to TAS for VMs v2.11 to successfully upgrade to TAS for VMs v2.13.

* Some partner service tiles might be incompatible with TAS for VMs v2.13.
  VMware is working with partners to ensure their tiles are updated to work with the latest versions of TAS for VMs.
  <br/><br/> For information about which partner service releases are currently compatible with TAS for VMs
  v2.13, review the appropriate partners services release documentation
  at [https://docs.pivotal.io](https://docs.pivotal.io) or contact the partner organization that produces the tile.


## <a id='new-features'></a> New Features in TAS for VMs v2.13

TAS for VMs v2.13 includes the following major features:

### <a id='scaling-factors'><a/>Configure Scaling Factors for App Autoscaler

In TAS for VMs v2.13.0, App Autoscaler can scale apps by one or more app instances based on autoscaling rules.
In previous releases of TAS for VMs, App Autoscaler only scaled app instances up or down by one instance at a time.
You can configure scaling factors using the App Autoscaler API, the App Autoscaler CLI plugin, or Apps Manager.

To configure scaling factors using the API, see [Update an App Binding](https://docs.pivotal.io/application-service/2-13/appsman-services/autoscaler/using-autoscaler-api.html#update-app) in _Using the App Autoscaler API_.

To configure scaling factors using the App Autoscaler CLI plugin, see [Update Scaling Factors](https://docs.pivotal.io/application-service/2-13/appsman-services/autoscaler/using-autoscaler-cli.html#updating-scaling-factors) in _Using the App Autoscaler CLI_.

To configure scaling factors using Apps Manager, see [Configure Autoscaling for an App](https://docs.pivotal.io/application-service/2-13/appsman-services/autoscaler/using-autoscaler.html#config) in _Scaling an App Using App Autoscaler_.

For more information, see [About App Autoscaler](https://docs.pivotal.io/application-service/2-13/appsman-services/autoscaler/about-app-autoscaler.html).

### <a id='offline-docs-errand'></a>Post-Deploy Errand for Offline Docs

TAS for VMs v2.13.0 adds the post-deploy **Offline Docs Errand**.
This errand pushes an offline docs app that can be used in air-gapped environments.

### <a id='separate-log-cache'></a>Log Cache Uses Its Own Instance Group

In TAS for VMs v2.13.0, the Log Cache component runs on its own Log Cache instance group. Log Cache is no longer part of Doppler instances. Additionally, syslog ingress is enabled by default.

This change allows TAS for VMs to do the following:

* Enable Log Cache to scale independently from Doppler. For example, to provide more memory for storing logs and metrics.
* Scale Log Cache to more than 40 VMs, because it is no longer limited by Doppler and TrafficController's throughput and connection-based horizontal scaling limits.
* Reduce the number of network hops required for logs and metrics to get to Log Cache.

For important information, see the related [Breaking Change](#separate-log-cache-breaking).

### <a id='mysql-8'></a> TAS for VMs is Compatible with MySQL 8

TAS for VMs v2.13.0 supports MySQL 8 for external databases.
Previous versions of TAS for VMs supported MySQL 5.7, which is expected to go out of support in 2023.
If you use an external MySQL database with TAS for VMs v2.13, use a MySQL 8 database for improved security and a longer window of support.

### <a id='golang-1-17'></a> TAS for VMs Components Use Golang v1.17

In TAS for VMs v2.13.0, several TAS for VMs components use Golang v1.17.

The following table lists the component releases that have been upgraded to use Golang v1.17:

<table class="nice">
  <tr>
    <th>Release Name</th>
    <th>First version that uses Golang v1.17</th>
  </tr>
  <tr>
    <td>cf-networking</td>
    <td>2.43.0</td>
  </tr>
  <tr>
    <td>silk</td>
    <td>2.43.0</td>
  </tr>
  <tr>
    <td>diego</td>
    <td>2.53.1</td>
  </tr>
  <tr>
    <td>nats</td>
    <td>41</td>
  </tr>
  <tr>
    <td>garden-runc</td>
    <td>1.19.31</td>
  </tr>
  <tr>
    <td>routing</td>
    <td>0.229.0</td>
  </tr>
</table>

For important information, see [Gorouter Certificates Require a SAN Extension](#certs-require-san) in the _Breaking Changes_ section.


## <a id='breaking-changes'></a> Breaking Changes

TAS for VMs v2.13 includes the following breaking changes:

### <a id='certs-require-san'></a> Gorouter Certificates Require a SAN Extension

In TAS for VMs v2.13, all Gorouter certificates require a valid SubjectAltName (SAN) extension.
If any Gorouter certificates lack a SubjectAltName, Go clients cannot connect to servers and deployment fails.

Before you upgrade to TAS for VMs v2.13, you must:

  1. Verify that all of your certificates in Ops Manager use a valid SAN. If they do not, rotate your certificates using valid SAN.
  1. Verify that all external systems that the Gorouter connects to also have certificates with a valid SAN. If you use route services, this includes either the route services themselves or the load balancer in front of the route service.

If you need to complete a deployment before configuring new Gorouter certificates, select **Enable temporary workaround for certs without SANs** in the **Networking** pane of the TAS for VMs tile.

For more information about updating certificates, see [Routing and Golang 1.15 X.509 CommonName deprecation](https://community.pivotal.io/s/article/Routing-and-golang-1-15-X-509-CommonName-deprecation) in the Knowledge Base.

### <a id='separate-log-cache-breaking'></a>Log Cache Uses Its Own Instance Group

In TAS for VMs v2.13, the Log Cache component runs on its own Log Cache instance group. Log Cache is no longer part of Doppler instances.

This change comes with the following considerations when you upgrade to TAS for VMs v2.13:

* Be sure to scale up your Log Cache instance count. VMware recommends scaling up to match the number of VMs and amount of memory as your Doppler instances before the upgrade.
  Underscaled Log Cache instances can fail with `Out of Memory` errors, which locks the BOSH Director. Starting larger and then adjusting for actual use is safer than a deployment failure.
* With Log Cache as its own instance group, you can reduce the memory allocation for Doppler instances.
* Log Cache uses syslog ingress by default. Confirm that your Diego Cells, including any Isolation Segment Diego Cells, are permitted to establish connections to Log Cache nodes on port 6067.
  You might need to update firewall rules to allow logs to flow directly from your Diego Cells to the Log Cache Syslog Server.
* You might experience temporary difficulties getting logs and metrics from the cf CLI for apps that are pushed during this upgrade.
  This should not affect whether or not the app is able to be deployed.
* Service instance metrics might not be retrievable using the `log-cache` cf CLI plugin.
* Diego Cells with high logging volume might experience higher CPU usage than they did prior to this change.


## <a id='known-issues'></a> Known Issues

TAS for VMs v2.13 includes the following known issues:
